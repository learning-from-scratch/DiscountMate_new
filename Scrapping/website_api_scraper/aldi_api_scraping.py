# -*- coding: utf-8 -*-
"""aldi_api_scraping.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1dhdvCgMdsCjYA4JD2scQCGAW3FTZBoBK
"""

"""
ALDI Australia Product Scraper
--------------------------------
This script:
- Downloads the latest ALDI category sitemap
- Scrapes all product data via ALDI's product-search API
- Dynamically flattens product JSON (handles category-specific fields)
- Normalises column names for clean data management
- Optionally downloads product images (ON/OFF switch)
  . Safe re-runs (skips existing images)
  . Atomic writes (prevents corrupted files)
- Exports everything to a single CSV

Designed for large-scale scraping and cross-retailer product matching.
"""

import os
import re
import json
import time
import random
from datetime import datetime
from typing import Any, Dict, List, Optional, Tuple

import requests
import pandas as pd
import xml.etree.ElementTree as ET

# ============================================================
# CONFIGURATION
# ============================================================

# Always fetch the latest category sitemap before scraping
SITEMAP_URL = "https://www.aldi.com.au/sitemap_categories.xml"
SITEMAP_PATH = "sitemap_categories.xml"

# ALDI Product Search API
ALDI_URL = "https://api.aldi.com.au/v3/product-search"
ALDI_SERVICE_POINT = "G452"     # Store / fulfilment location identifier
ALDI_SERVICE_TYPE = "walk-in"   # In-store pricing
ALDI_CURRENCY = "AUD"
ALDI_LIMIT = 30                 # API allows only specific values

# Network & retry configuration
REQUEST_TIMEOUT_S = 25
MAX_RETRIES = 4
SLEEP_BETWEEN_CALLS = (0.6, 1.6)

# User-Agent and headers to mimic browser traffic
UA = (
    "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) "
    "AppleWebKit/605.1.15 (KHTML, like Gecko) Version/26.2 Safari/605.1.15"
)
COMMON_HEADERS = {
    "User-Agent": UA,
    "Accept": "application/json, text/plain, */*",
    "Accept-Language": "en-AU,en;q=0.9",
    "Origin": "https://www.aldi.com.au",
    "Referer": "https://www.aldi.com.au/",
}

# ============================================================
# IMAGE DOWNLOAD TOGGLE (ON / OFF)
# ============================================================

DOWNLOAD_IMAGES = True                  # Set True to enable image downloads
IMAGES_ROOT_DIR = "."                   # Images saved as: {retailer}/{sku}.jpg
IMAGE_WIDTH = 1000                      # Width injected into ALDI image URL
MAX_IMAGES_PER_RUN = None               # Optional cap to avoid large downloads
SLEEP_BETWEEN_IMAGE_DOWNLOADS = (0.05, 0.25)

# ============================================================
# GENERAL HELPERS
# ============================================================

def now_ts() -> str:
    """Return current timestamp in a consistent format."""
    return datetime.now().strftime("%Y-%m-%d %H:%M:%S")


def safe_str(x: Any) -> str:
    """Convert None to empty string, otherwise cast to str."""
    return "" if x is None else str(x)


def cents_to_dollars(cents: Any) -> Optional[float]:
    """Convert integer cents to dollar float."""
    if isinstance(cents, bool):
        return None
    if isinstance(cents, (int, float)):
        return round(float(cents) / 100.0, 2)
    if isinstance(cents, str) and cents.strip().isdigit():
        return round(int(cents.strip()) / 100.0, 2)
    return None


def retailer_slug(name: str) -> str:
    """Convert retailer display name into a stable folder name."""
    s = safe_str(name).strip().lower()
    s = re.sub(r"[^a-z0-9]+", "-", s).strip("-")
    return s or "retailer"


def ensure_dir(path: str) -> None:
    """Create directory if it does not exist."""
    os.makedirs(path, exist_ok=True)

# ============================================================
# SITEMAP HANDLING
# ============================================================

def download_sitemap(url: str, path: str) -> str:
    """
    Download the latest sitemap_categories.xml from ALDI.
    Falls back to local file if download fails.
    """
    try:
        r = requests.get(
            url,
            headers={"User-Agent": UA, "Accept": "application/xml,text/xml,*/*"},
            timeout=REQUEST_TIMEOUT_S,
        )
        r.raise_for_status()

        text = r.text.strip()
        if "<urlset" not in text and "<sitemapindex" not in text:
            raise ValueError("Downloaded content does not look like a sitemap XML")

        with open(path, "wb") as f:
            f.write(r.content)

        print(f"Sitemap downloaded. bytes={len(r.content)}. saved={path}")
        return path
    except Exception as e:
        if os.path.exists(path):
            print(f"Sitemap download failed ({type(e).__name__}). Using existing local file: {path}")
            return path
        raise


def parse_sitemap_category_urls(xml_path: str) -> List[str]:
    """Parse category URLs from the sitemap XML."""
    tree = ET.parse(xml_path)
    root = tree.getroot()
    ns = {"sm": "http://www.sitemaps.org/schemas/sitemap/0.9"}

    urls: List[str] = []
    for url_node in root.findall("sm:url", ns):
        loc = url_node.find("sm:loc", ns)
        if loc is not None and loc.text:
            urls.append(loc.text.strip())

    seen = set()
    out: List[str] = []
    for u in urls:
        if u not in seen:
            out.append(u)
            seen.add(u)
    return out


def category_id_from_url(url: str) -> Optional[str]:
    """Extract ALDI category ID from sitemap URL."""
    m = re.search(r"/k/(\d+)", url)
    return m.group(1) if m else None

# ============================================================
# API REQUEST HELPERS
# ============================================================

def request_json(url: str, headers: Dict[str, str], params: Dict[str, Any]) -> Tuple[Optional[Dict[str, Any]], str]:
    """
    Make a GET request and return parsed JSON + status string.
    Handles common HTTP blocks and invalid JSON.
    """
    try:
        r = requests.get(url, headers=headers, params=params, timeout=REQUEST_TIMEOUT_S)

        if r.status_code == 429:
            return None, "RATE_LIMITED_429"
        if r.status_code == 403:
            return None, "BLOCKED_403"
        if r.status_code != 200:
            return None, f"HTTP_{r.status_code}"

        try:
            return r.json(), "SUCCESS"
        except Exception:
            return None, "INVALID_JSON"

    except requests.exceptions.Timeout:
        return None, "TIMEOUT"
    except requests.exceptions.ConnectionError:
        return None, "CONNECTION_ERROR"
    except Exception:
        return None, "ERROR_UNKNOWN"


def extract_items_from_response(payload: Dict[str, Any]) -> List[Dict[str, Any]]:
    """
    Extract a list of product items from an API response.
    Current ALDI shape: payload["data"] is a list of product dicts.
    """
    data = payload.get("data")
    if isinstance(data, list):
        return [x for x in data if isinstance(x, dict)]
    if isinstance(data, dict):
        for k in ["items", "products", "results"]:
            v = data.get(k)
            if isinstance(v, list):
                return [x for x in v if isinstance(x, dict)]
    return []


def extract_total_count_from_response(payload: Dict[str, Any]) -> Optional[int]:
    """Extract totalCount from a response when available."""
    meta = payload.get("meta") or {}
    pag = meta.get("pagination") or {}
    tc = pag.get("totalCount")
    return tc if isinstance(tc, int) else None

# ============================================================
# IMAGE UTILITIES
# ============================================================

def build_image_url(asset_url_template: str, slug: str, width: int = IMAGE_WIDTH) -> str:
    """
    Inject width and slug into ALDI image URL template.
    Example template:
      https://.../scaleWidth/{width}/<uuid>/{slug}
    """
    u = safe_str(asset_url_template).replace("{width}", str(width))
    if "{slug}" in u:
        u = u.replace("{slug}", safe_str(slug) if slug else "")
    return u


def build_product_url_from_slug(slug: str) -> str:
    """Best-effort product page URL."""
    slug = safe_str(slug).strip().strip("/")
    if not slug:
        return ""
    return f"https://www.aldi.com.au/groceries/{slug}/"


def pick_asset(assets: Any, preferred_types: Tuple[str, ...] = ("FR01", "FR02", "NU01", "IN01")) -> Dict[str, Any]:
    """Pick a preferred image asset from an assets list."""
    if not isinstance(assets, list):
        return {}
    typed = [a for a in assets if isinstance(a, dict) and a.get("assetType")]
    for t in preferred_types:
        for a in typed:
            if a.get("assetType") == t:
                return a
    return typed[0] if typed else {}


def image_paths_for_website(retailer: str, sku: str) -> Tuple[str, str]:
    """
    Returns (local_path, website_relative_path) for an image.
    Target structure: {retailer}/{sku}.jpg
    """
    r = retailer_slug(retailer)
    sku = safe_str(sku).strip()

    website_rel = f"{r}/{sku}.jpg"
    local_path = os.path.join(IMAGES_ROOT_DIR, r, f"{sku}.jpg")

    return local_path, website_rel


def download_image_if_needed(
    session: requests.Session,
    image_url: str,
    retailer: str,
    sku: str,
) -> Tuple[bool, str, str]:
    """
    Download an image only if it does not already exist.

    Guarantees:
    - Safe re-runs. Skips existing non-empty files
    - Atomic writes. Downloads to .part then replaces final
    - Website friendly naming. {retailer}/{sku}.jpg

    Returns (downloaded, local_path, website_relative_path).
    """
    if not image_url:
        return False, "", ""
    sku = safe_str(sku).strip()
    if not sku:
        return False, "", ""

    local_path, website_rel = image_paths_for_website(retailer, sku)
    ensure_dir(os.path.dirname(local_path))

    # Safe re-run. If already downloaded and non-empty, skip
    if os.path.exists(local_path) and os.path.getsize(local_path) > 0:
        return False, local_path, website_rel

    tmp_path = local_path + ".part"

    try:
        with session.get(image_url, timeout=REQUEST_TIMEOUT_S, stream=True) as r:
            if r.status_code != 200:
                return False, "", ""

            with open(tmp_path, "wb") as f:
                for chunk in r.iter_content(chunk_size=1024 * 64):
                    if chunk:
                        f.write(chunk)

        # Atomic replace
        os.replace(tmp_path, local_path)

        # Sanity check
        if os.path.getsize(local_path) == 0:
            try:
                os.remove(local_path)
            except Exception:
                pass
            return False, "", ""

        return True, local_path, website_rel

    except Exception:
        try:
            if os.path.exists(tmp_path):
                os.remove(tmp_path)
        except Exception:
            pass
        return False, "", ""

# ============================================================
# JSON FLATTENING (DYNAMIC FIELDS)
# ============================================================

def flatten_any(
    obj: Any,
    parent_key: str = "",
    sep: str = ".",
    keep_lists_as_json: bool = True,
    max_depth: int = 20,
    _depth: int = 0
) -> Dict[str, Any]:
    """
    Recursively flatten nested JSON structures.
    Lists are stored as JSON strings to prevent column explosion.
    """
    out: Dict[str, Any] = {}
    if _depth > max_depth:
        out[parent_key] = json.dumps(obj, ensure_ascii=False)
        return out

    if isinstance(obj, dict):
        for k, v in obj.items():
            new_key = f"{parent_key}{sep}{k}" if parent_key else str(k)
            out.update(flatten_any(v, new_key, sep, keep_lists_as_json, max_depth, _depth + 1))
        return out

    if isinstance(obj, list):
        if keep_lists_as_json:
            out[parent_key] = json.dumps(obj, ensure_ascii=False)
            return out
        for i, v in enumerate(obj):
            new_key = f"{parent_key}{sep}{i}" if parent_key else str(i)
            out.update(flatten_any(v, new_key, sep, keep_lists_as_json, max_depth, _depth + 1))
        return out

    out[parent_key] = obj
    return out

# ============================================================
# COLUMN NAMING (SNAKE CASE + UNIQUE)
# ============================================================

CORE_RENAME = {
    "Retailer": "retailer",
    "CategoryId_FromSitemap": "category_id_from_sitemap",
    "CategoryUrl_FromSitemap": "category_url_from_sitemap",
    "SKU": "sku",
    "Name": "name",
    "BrandName": "brand_name",
    "UrlSlugText": "url_slug",
    "ProductUrl": "product_url",
    "ImageUrl": "image_url",
    "PriceCents": "price_cents",
    "PriceDollars": "price_dollars",
    "PriceDisplay": "price_display",
    "Timestamp": "scraped_at",
    "RawJson": "raw_json",
    "ImageLocalPath": "image_local_path",
    "ImageWebPath": "image_web_path",
}

def to_snake(s: str) -> str:
    """Convert a column name to snake_case."""
    s = str(s).strip()
    s = re.sub(r"[^\w\.]+", "_", s)
    s = re.sub(r"([a-z0-9])([A-Z])", r"\1_\2", s)
    s = s.replace(".", "_")
    s = re.sub(r"_+", "_", s).strip("_")
    return s.lower()

def format_column_name(col: str) -> str:
    """Normalise column names for easier downstream data handling."""
    col = str(col)
    if col.startswith("aldi."):
        return to_snake(col)
    if col in CORE_RENAME:
        return CORE_RENAME[col]
    return to_snake(col)

def rename_columns_for_management(df: pd.DataFrame) -> pd.DataFrame:
    """Apply column formatting and ensure uniqueness."""
    new_cols = [format_column_name(c) for c in df.columns]
    seen: Dict[str, int] = {}
    final_cols: List[str] = []
    for c in new_cols:
        if c not in seen:
            seen[c] = 1
            final_cols.append(c)
        else:
            seen[c] += 1
            final_cols.append(f"{c}_{seen[c]}")
    df = df.copy()
    df.columns = final_cols
    return df

# ============================================================
# PRODUCT EXTRACTION (CORE + FLATTENED)
# ============================================================

def extract_aldi_product_dynamic(
    item: Dict[str, Any],
    category_id: str,
    category_url: str,
    session: Optional[requests.Session] = None,
    download_images: bool = False,
    retailer_name: str = "ALDI"
) -> Dict[str, Any]:
    """
    Extract core product fields plus dynamically flattened fields.

    If download_images is enabled:
    - downloads a primary image to images/{retailer}/{sku}.jpg
    - skips if already downloaded
    """
    sku = item.get("sku") or item.get("id") or item.get("productId") or ""
    name = item.get("name") or item.get("title") or item.get("displayName") or ""
    brand = item.get("brandName") or item.get("brand") or ""
    slug = item.get("urlSlugText") or ""

    pricing = item.get("price") or {}
    amount_cents = pricing.get("amount") if isinstance(pricing, dict) else None
    amount_dollars = cents_to_dollars(amount_cents)
    amount_display = safe_str(pricing.get("amountRelevantDisplay") or pricing.get("amountDisplay")) if isinstance(pricing, dict) else ""

    assets = item.get("assets") or []
    chosen_asset = pick_asset(assets)
    image_url = ""
    if chosen_asset and chosen_asset.get("url"):
        image_url = build_image_url(chosen_asset["url"], slug, width=IMAGE_WIDTH)

    image_local_path = ""
    image_web_path = ""

    if download_images and image_url and session is not None:
        downloaded, local_path, web_rel = download_image_if_needed(
            session=session,
            image_url=image_url,
            retailer=retailer_name,
            sku=safe_str(sku),
        )
        if local_path:
            image_local_path = local_path
            image_web_path = web_rel

        time.sleep(random.uniform(*SLEEP_BETWEEN_IMAGE_DOWNLOADS))

    row_core = {
        "Retailer": retailer_name,
        "CategoryId_FromSitemap": category_id,
        "CategoryUrl_FromSitemap": category_url,

        "SKU": safe_str(sku),
        "Name": safe_str(name),
        "BrandName": safe_str(brand),
        "UrlSlugText": safe_str(slug),

        "ProductUrl": build_product_url_from_slug(slug),
        "ImageUrl": safe_str(image_url),
        "ImageLocalPath": safe_str(image_local_path),
        "ImageWebPath": safe_str(image_web_path),

        "PriceCents": amount_cents if amount_cents is not None else "",
        "PriceDollars": amount_dollars if amount_dollars is not None else "",
        "PriceDisplay": safe_str(amount_display),

        "Timestamp": now_ts(),
        "RawJson": json.dumps(item, ensure_ascii=False),
    }

    flat = flatten_any(item, parent_key="aldi", keep_lists_as_json=True)
    return {**flat, **row_core}

# ============================================================
# CORE SCRAPING
# ============================================================

def fetch_category_products(
    category_id: str,
    category_url: str,
    session: requests.Session,
    download_images: bool
) -> List[Dict[str, Any]]:
    """Fetch all products for a category by paging through offsets."""
    rows: List[Dict[str, Any]] = []
    offset = 0
    total_expected: Optional[int] = None

    while True:
        params = {
            "currency": ALDI_CURRENCY,
            "serviceType": ALDI_SERVICE_TYPE,
            "q": "",
            "limit": ALDI_LIMIT,
            "offset": offset,
            "sort": "relevance",
            "servicePoint": ALDI_SERVICE_POINT,
            "categoryTree": category_id,
        }

        payload = None
        status = "UNKNOWN"

        for attempt in range(MAX_RETRIES):
            payload, status = request_json(ALDI_URL, headers=COMMON_HEADERS, params=params)
            if status == "SUCCESS":
                break
            time.sleep((1.0 + attempt) * random.uniform(0.8, 1.6))

        if not payload or status != "SUCCESS":
            print(f"  category={category_id}. offset={offset}. FAIL. {status}")
            break

        if total_expected is None:
            total_expected = extract_total_count_from_response(payload)

        items = extract_items_from_response(payload)
        if not items:
            print(f"  category={category_id}. offset={offset}. no items. totalCount={extract_total_count_from_response(payload)}. stop")
            break

        for it in items:
            rows.append(
                extract_aldi_product_dynamic(
                    it,
                    category_id,
                    category_url,
                    session=session,
                    download_images=download_images,
                    retailer_name="ALDI",
                )
            )

        print(f"  category={category_id}. offset={offset}. got={len(items)}. total_so_far={len(rows)}")

        if len(items) < ALDI_LIMIT:
            break

        offset += ALDI_LIMIT
        time.sleep(random.uniform(*SLEEP_BETWEEN_CALLS))

        if isinstance(total_expected, int) and len(rows) >= total_expected:
            break

    return rows


def run_aldi_full_site_scrape(download_images: bool = DOWNLOAD_IMAGES) -> pd.DataFrame:
    """Orchestrate the full ALDI scrape."""
    download_sitemap(SITEMAP_URL, SITEMAP_PATH)

    category_urls = parse_sitemap_category_urls(SITEMAP_PATH)
    print(f"Loaded category urls: {len(category_urls)}")

    categories: List[Tuple[str, str]] = []
    for u in category_urls:
        cid = category_id_from_url(u)
        if cid:
            categories.append((cid, u))

    seen = set()
    categories_unique: List[Tuple[str, str]] = []
    for cid, u in categories:
        if cid not in seen:
            categories_unique.append((cid, u))
            seen.add(cid)

    print(f"Unique category ids: {len(categories_unique)}")

    session = requests.Session()

    all_rows: List[Dict[str, Any]] = []
    images_downloaded = 0

    for i, (cid, url) in enumerate(categories_unique, start=1):
        print(f"[{i}/{len(categories_unique)}] scraping categoryId={cid}")
        rows = fetch_category_products(
            category_id=cid,
            category_url=url,
            session=session,
            download_images=download_images,
        )
        all_rows.extend(rows)

        if download_images and MAX_IMAGES_PER_RUN is not None:
            for r in rows:
                web_path = r.get("ImageWebPath") or ""
                if web_path:
                    images_downloaded += 1
            if images_downloaded >= MAX_IMAGES_PER_RUN:
                print(f"Reached MAX_IMAGES_PER_RUN={MAX_IMAGES_PER_RUN}. Stopping early.")
                break

        time.sleep(random.uniform(1.0, 2.5))

    df_raw = pd.DataFrame(all_rows)
    df = rename_columns_for_management(df_raw)

    ts = datetime.now().strftime("%Y%m%d_%H%M%S")
    out_csv = f"aldi_all_products_{ts}.csv"
    df.to_csv(out_csv, index=False, encoding="utf-8-sig")

    print(f"\nDONE. rows={len(df)}. cols={len(df.columns)}. saved={out_csv}")
    return df

# ============================================================
# RUN
# ============================================================

df_aldi = run_aldi_full_site_scrape(download_images=DOWNLOAD_IMAGES)
df_aldi.head(5)

